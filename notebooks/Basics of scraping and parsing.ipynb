{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Самоучитель по Python для начинающих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 17: Основы скрапинга и парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование объекта datetime :\n",
    "from datetime import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регулярные выражения\n",
    "from urllib.request import urlopen\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы с HTML кодом в Python используют модуль etree:\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MechanicalSoup\n",
      "  Downloading MechanicalSoup-1.3.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.7 in c:\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.9.1)\n",
      "Requirement already satisfied: lxml in c:\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.5.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\anaconda3\\lib\\site-packages (from MechanicalSoup) (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.7->MechanicalSoup) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (3.0.4)\n",
      "Installing collected packages: MechanicalSoup\n",
      "Successfully installed MechanicalSoup-1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Библиотека для имитации действий пользователя:\n",
    "\n",
    "!pip install MechanicalSoup\n",
    "# import mechanicalsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы с динамическим контентом в Python нужно установить:\n",
    "\n",
    "# - Модуль Selenium.\n",
    "# - Драйвер Selenium WebDriver для браузера.\n",
    "\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.14.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Библиотека для имитации действий пользователя:\n",
    "\n",
    "# !pip install selenium\n",
    "# import selenium\n",
    "selenium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ChromeDriverManager in c:\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: pywin32 in c:\\anaconda3\\lib\\site-packages (from ChromeDriverManager) (227)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from ChromeDriverManager) (2.24.0)\n",
      "Requirement already satisfied: pypiwin32 in c:\\anaconda3\\lib\\site-packages (from ChromeDriverManager) (223)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install ChromeDriverManager\n",
    "# ChromeDriverManager.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChromeDriverManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0ef7914e43a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ChromeDriverManager' is not defined"
     ]
    }
   ],
   "source": [
    "ChromeDriverManager.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement webdriver_manager.chrome (from versions: none)\n",
      "ERROR: No matching distribution found for webdriver_manager.chrome\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager.chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webdriver_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e2f2071796ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwebdriver_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchrome\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'webdriver_manager'"
     ]
    }
   ],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бывает, что даже после установки оптимальной версии драйвера интерпретатор Python возвращает ошибку OSError:\n",
    "# [WinError 216] Версия \"%1\" не совместима с версией Windows.\n",
    "# В этом случае нужно воспользоваться модулем webdriver-manager,\n",
    "# который самостоятельно установит подходящий драйвер для нужного браузера:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use with Chrome\n",
    "# selenium 3\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium 4\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполняем предыдущую ячейку ПОСТРОЧНО !!!\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service as ChromeService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.24.0)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from webdriver-manager) (20.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (2.4.7)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install webdriver-manager\n",
    "# from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'service'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-9e67e88712cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mChromeService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'service'"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научимся извлекать данные из статического и динамического контента с помощью:\n",
    "регулярных выражений, XPath, BeautifulSoup, MechanicalSoup и Selenium.\n",
    "\n",
    "В конце статьи – код 10 скриптов для скрапинга данных и изображений с Wikipedia, Habr, LiveLib, IMDb и TIOBE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веб-скрапинг – это процесс автоматического сбора информации из онлайн-источников.\n",
    "Для выбора нужных сведений из массива «сырых» данных, полученных в ходе скрапинга, нужна дальнейшая обработка – парсинг.\n",
    "В процессе парсинга выполняются синтаксический анализ, разбор и очистка данных.\n",
    "Результат парсинга – очищенные, упорядоченные, структурированные данные, представленные в формате, понятном конечному пользователю (или приложению)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипты для скрапинга создают определенную нагрузку на сайт, с которого они собирают данные – могут, например, посылать чрезмерное количество GET запросов к серверу.\n",
    "Это одна из причин, по которой скрапинг относится к спорным видам деятельности.\n",
    "Чтобы не выходить за рамки сетевого этикета, необходимо всегда соблюдать главные правила сбора публичной информации:\n",
    "\n",
    "- Если на сайте есть API, нужно запрашивать данные у него.\n",
    "- Частота и количество GET запросов должны быть разумными.\n",
    "- Следует передавать информацию о клиенте в User-Agent.\n",
    "- Если на сайте есть личные данные пользователей, необходимо учитывать настройки приватности в robots.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо отметить, что универсальных рецептов скрапинга и парсинга не существует.\n",
    "Это связано со следующими причинами:\n",
    "\n",
    "- Некоторые сервисы активно блокируют скраперов. Динамическая смена прокси не всегда помогает решить эту проблему.\n",
    "\n",
    "- Контент многих современных сайтов генерируется динамически – результат обычного GET запроса из приложения к таким сайтам вернется практически пустым. Эта проблема решается с помощью Selenium WebDriver либо MechanicalSoup, которые имитируют действия браузера и пользователя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для извлечения данных со страниц с четкой, стандартной структурой эффективнее использовать язык запросов XPath.\n",
    "\n",
    "И напротив, для получения нужной информации с нестандартных страниц с произвольным синтаксисом лучше использовать средства библиотеки BeautifulSoup.\n",
    "\n",
    "Ниже мы подробно рассмотрим оба подхода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры скрапинга и парсинга на Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экосистема Python располагает множеством инструментов для скрапинга и парсинга.\n",
    "Начнем с самого простого примера – получения веб-страницы и извлечения из ее кода ссылки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скрапинг содержимого страницы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://example.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Воспользуемся модулем urllib.request стандартной библиотеки urllib\n",
    "# для получения исходного кода одностраничного сайта example.com:\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = 'http://example.com'\n",
    "page = urlopen(url)\n",
    "\n",
    "print(page.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Example Domain</title>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "<style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "<div>\n",
      "<h1>Example Domain</h1>\n",
      "<p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "<p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Точно такой же результат можно получить с помощью requests:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://example.com'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот результат – те самые сырые данные, которые нужно обработать (подвергнуть парсингу), чтобы извлечь из них нужную информацию, например, адрес указанной на странице ссылки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Парсинг полученных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечь адрес ссылки можно 4 разными способами – с помощью:\n",
    "\n",
    "- Методов строк.\n",
    "- Регулярного выражения.\n",
    "- Запроса XPath.\n",
    "- Обработки BeautifulSoup.\n",
    "\n",
    "Рассмотрим все эти способы по порядку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы строк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это самый трудоемкий способ – для извлечения каждого элемента нужно определить 2 индекса – начало и конец вхождения.\n",
    "При этом к индексу вхождения надо добавить длину стартового фрагмента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167\n",
      "1203\n",
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# \n",
    "url = 'http://example.com/'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "\n",
    "# \n",
    "start = html_code.find('href=\"') + 6\n",
    "end = html_code.find('\">More')\n",
    "link = html_code[start:end]\n",
    "\n",
    "print(start)\n",
    "print(end)\n",
    "\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Регулярное выражение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущей главе мы подробно рассматривали способы извлечения конкретных подстрок из текста.\n",
    "https://proglib.io/p/samouchitel-po-python-dlya-nachinayushchih-chast-16-regulyarnye-vyrazheniya-2023-02-20\n",
    "\n",
    "Точно так же регулярные выражения можно использовать для поиска данных в исходном коде страниц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.iana.org/domains/example']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "# \n",
    "url = 'http://example.com/'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "\n",
    "# \n",
    "link = r'(https?://\\S+)(?=\")'\n",
    "\n",
    "print(re.findall(link, html_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запрос XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Язык запросов XPath (XML Path Language) позволяет извлекать данные из определенных узлов XML-документа.\n",
    "Для работы с HTML кодом в Python используют модуль etree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "\n",
    "# \n",
    "url = 'http://example.com/'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "tree = etree.HTML(html_code)\n",
    "\n",
    "print(tree.xpath(\"/html/body/div/p[2]/a/@href\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы узнать путь к нужному элементу страницы, в браузерах Chrome и FireFox надо кликнуть правой кнопкой по элементу и выбрать «Просмотреть код», после чего откроется консоль.\n",
    "В консоли по интересующему элементу нужно еще раз кликнуть правой кнопкой, выбрать «Копировать», а затем – копировать путь XPath:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В приведенном выше примере для извлечения ссылки к пути /html/body/div/p[2]/a/\n",
    "мы добавили указание для получения значения ссылки @href, и индекс [0], поскольку результат возвращается в виде списка.\n",
    "\n",
    "###### Если @href заменить на text(), программа вернет текст ссылки, а не сам URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More information...\n"
     ]
    }
   ],
   "source": [
    "print(tree.xpath(\"/html/body/div/p[2]/a/text()\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсер BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярные выражения и XPath предоставляют огромные возможности для извлечения нужной информации из кода страниц, но у них есть свои недостатки:\n",
    "составлять Regex-шаблоны сложно, а запросы XPath хорошо работают только на страницах с безупречной, стандартной структурой.\n",
    "К примеру, страницы Википедии не отличаются идеальной структурой, и использование XPath для извлечения нужной информации из определенных элементов статей, таких как таблицы infobox, часто оказывается неэффективным.\n",
    "\n",
    "##### В этом случае оптимальным вариантом становится BeautifulSoup, специально разработанный для парсинга HTML-кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Библиотека BeautifulSoup не входит в стандартный набор Python, ее нужно установить самостоятельно:\n",
    "        \n",
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В приведенном ниже примере мы будем извлекать из исходного кода страницы уникальные ссылки, за исключением внутренних:\n",
    "\n",
    "https://webscraper.io/test-sites/e-commerce/allinone/phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/webscraperio\n",
      "https://forum.webscraper.io/\n",
      "https://cloud.webscraper.io/\n",
      "https://www.facebook.com/webscraperio/\n",
      "https://chromewebstore.google.com/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en\n",
      "https://youtube.com/@WebScraper/videos\n",
      "https://webscraper.io/downloads/Web_Scraper_Media_Kit.zip\n",
      "https://lv.linkedin.com/company/web-scraper\n",
      "https://status.webscraper.io/\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# \n",
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/phones'\n",
    "page = urlopen(url)\n",
    "html = page.read().decode('utf-8')\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# \n",
    "links = set()\n",
    "for link in soup.find_all('a'):\n",
    "    l = link.get('href')\n",
    "    if l != None and l.startswith('https'):\n",
    "        links.add(l)\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/webscraperio\n",
      "https://forum.webscraper.io/\n",
      "https://cloud.webscraper.io/\n",
      "https://www.facebook.com/webscraperio/\n",
      "https://chromewebstore.google.com/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en\n",
      "https://youtube.com/@WebScraper/videos\n",
      "https://webscraper.io/downloads/Web_Scraper_Media_Kit.zip\n",
      "https://lv.linkedin.com/company/web-scraper\n",
      "https://status.webscraper.io/\n"
     ]
    }
   ],
   "source": [
    "# При использовании XPath точно такой же результат даст следующий скрипт:\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "\n",
    "# \n",
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/phones'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "tree = etree.HTML(html_code)\n",
    "sp = tree.xpath(\"//li/a/@href\")\n",
    "\n",
    "# \n",
    "links = set()\n",
    "for link in sp:\n",
    "    if link.startswith('http'):\n",
    "        links.add(link)\n",
    "\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имитация действий пользователя в браузере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При скрапинге сайтов очень часто требуется авторизация, нажатие кнопок «Читать дальше», переход по ссылкам, отправка форм, прокручивание ленты и так далее.\n",
    "Отсюда возникает необходимость имитации действий пользователя.\n",
    "Как правило, для этих целей используют Selenium, однако есть и более легкое решение – библиотека MechanicalSoup:\n",
    "\n",
    "###### pip install MechanicalSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, MechanicalSoup исполняет роль браузера без графического интерфейса.\n",
    "Помимо имитации нужного взаимодействия с элементами страниц,\n",
    "MechanicalSoup также парсит HTML-код, используя для этого все функции BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся тестовым сайтом http://httpbin.org/, на котором есть возможность отправки формы заказа пиццы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<input name=\"custname\"/>\n",
      "<input name=\"custtel\" type=\"tel\"/>\n",
      "<input name=\"custemail\" type=\"email\"/>\n",
      "<input name=\"size\" type=\"radio\" value=\"small\"/>\n",
      "<input name=\"size\" type=\"radio\" value=\"medium\"/>\n",
      "<input name=\"size\" type=\"radio\" value=\"large\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"bacon\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"cheese\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"onion\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"mushroom\"/>\n",
      "<input max=\"21:00\" min=\"11:00\" name=\"delivery\" step=\"900\" type=\"time\"/>\n",
      "<textarea name=\"comments\"></textarea>\n",
      "<button>Submit order</button>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "\n",
    "# \n",
    "browser = mechanicalsoup.StatefulBrowser()\n",
    "browser.open(\"http://httpbin.org/\")\n",
    "browser.follow_link(\"forms\")\n",
    "browser.select_form('form[action=\"/post\"]')\n",
    "\n",
    "print(browser.form.print_summary())\n",
    "\n",
    "# В приведенном выше примере браузер MechanicalSoup перешел по внутренней ссылке http://httpbin.org/forms/post\n",
    "# и вернул описание полей ввода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перейдем к имитации заполнения формы:\n",
    "        \n",
    "browser[\"custname\"] = \"Best Customer\"\n",
    "browser[\"custtel\"] = \"+7 916 123 45 67\"\n",
    "browser[\"custemail\"] = \"trex@example.com\"\n",
    "browser[\"size\"] = \"large\"\n",
    "browser[\"topping\"] = (\"cheese\", \"mushroom\")\n",
    "browser[\"comments\"] = \"Add more cheese, plz. More than the last time!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь форму можно отправить:\n",
    "        \n",
    "response = browser.submit_selected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"comments\": \"Add more cheese, plz. More than the last time!\", \n",
      "    \"custemail\": \"trex@example.com\", \n",
      "    \"custname\": \"Best Customer\", \n",
      "    \"custtel\": \"+7 916 123 45 67\", \n",
      "    \"delivery\": \"\", \n",
      "    \"size\": \"large\", \n",
      "    \"topping\": [\n",
      "      \"cheese\", \n",
      "      \"mushroom\"\n",
      "    ]\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"191\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"Referer\": \"http://httpbin.org/forms/post\", \n",
      "    \"User-Agent\": \"python-requests/2.24.0 (MechanicalSoup/1.3.0)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-67155811-78bd5d122a1637122c7af07f\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"94.25.225.163\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Результат можно вывести с помощью print(response.text):\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скрапинг и парсинг динамического контента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все примеры, которые мы рассмотрели выше, отлично работают на статических страницах.\n",
    "Однако на множестве платформ используется динамический подход к генерации и загрузке контента –\n",
    "к примеру, для просмотра всех доступных товаров в онлайн-магазине страницу нужно не только открыть, но и прокрутить до футера. Для работы с динамическим контентом в Python нужно установить:\n",
    "\n",
    "- Модуль Selenium.\n",
    "- Драйвер Selenium WebDriver для браузера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ ИСПОЛЬЗОВАТЬ  ДАННЫЙ  КОД  ДЛЯ  ПАРСИНГА  САЙТОВ $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "# Use with Chrome\n",
    "# selenium 3\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ ИСПОЛЬЗОВАТЬ  ДАННЫЙ  КОД  ДЛЯ  ПАРСИНГА  САЙТОВ $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если установка прошла успешно, выполнение этого кода приведет к автоматическому открытию страницы:\n",
    "# from selenium import webdriver\n",
    "\n",
    "\n",
    "# driver = webdriver.Chrome() # или webdriver.Firefox()\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get('https://google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Имитация прокрутки страницы и парсинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера скрапинга и парсинга динамического сайта мы воспользуемся разделом тестового онлайн-магазина.\n",
    "Здесь расположены карточки с информацией о планшетах.\n",
    "Карточки загружаются ряд за рядом при прокрутке страницы:\n",
    "\n",
    "https://webscraper.io/test-sites/e-commerce/scroll/computers/tablets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока страница не прокручена, полный HTML-код с информацией о планшетах получить невозможно.\n",
    "Для имитации прокрутки мы воспользуемся скриптом 'window.scrollTo(0, document.body.scrollHeight);'.\n",
    "Цены планшетов находятся в тегах \"H4\" класса pull-right price, а названия моделей – в тексте ссылок \"A\" класса title.\n",
    "Готовый код выглядит так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_line_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             self.process = subprocess.Popen(cmd, env=self.env,\n\u001b[0m\u001b[0;32m     73\u001b[0m                                             \u001b[0mclose_fds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Windows'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Не удается найти указанный файл",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-5a62c0f09c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# driver = webdriver.Chrome(service=service)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:\\Program Files\\Chrome Driver\\chromedriver.exe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://webscraper.io/test-sites/e-commerce/scroll/computers/tablets'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             log_path=service_log_path)\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 raise WebDriverException(\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m     83\u001b[0m                         os.path.basename(self.path), self.start_error_message)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Оригинальная функция к данной задаче \n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager(cache_valid_range=10).install()))\n",
    "\n",
    "# Функция полученная несколько ячеек выше с добавлением \"cache_valid_range=10\":\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(ChromeDriverManager(cache_valid_range=10).install())\n",
    "\n",
    "\n",
    "url = 'https://webscraper.io/test-sites/e-commerce/scroll/computers/tablets'\n",
    "driver.get(url)\n",
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "time.sleep(5) \n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "prices = soup.find_all('h4', class_='pull-right price')\n",
    "models = soup.find_all('a', class_='title')\n",
    "\n",
    "for model, price in zip(models, prices):\n",
    "    m = model.get_text()\n",
    "    p = price.get_text()\n",
    "    print(f'Планшет {m}, цена - {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для получения названий последних статей из блога издательства O’Reilly:\n",
    "\n",
    "https://www.oreilly.com/radar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI in the Real World: The Future of Programming with Matt Welsh\n",
      "The State of Security in 2024\n",
      "Beyond Imitation\n",
      "Henry Ford Does AI\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "url = 'https://www.oreilly.com/radar/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "sp = soup.find_all('h2', class_='post-title')\n",
    "for post in sp:\n",
    "    print(post.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для определения 10 слов, которые чаще всего встречаются в тексте сказки «Колобок» (без учета регистра). Предлоги учитывать не нужно.\n",
    "\n",
    "https://azku.ru/russkie-narodnie-skazki/kolobok.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "колобок - 32\n",
      "ушел - 14\n",
      "тебя - 7\n",
      "коробу - 6\n",
      "сусеку - 6\n",
      "сметане - 5\n",
      "масле - 5\n",
      "покатился - 4\n",
      "катится - 4\n",
      "навстречу - 4\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "url = 'https://azku.ru/russkie-narodnie-skazki/kolobok.html'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "res = soup.find_all('div', class_='entry-content')\n",
    "text = res[0].get_text().split('Мне нравится')[0]\n",
    "sp = re.sub(r'[-,.?!\"—:]', ' ', text).lower().split()\n",
    "\n",
    "result_dict = {word: sp.count(word) for word in sp if len(word) > 3}\n",
    "max_values = sorted(result_dict.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "\n",
    "for word, number in max_values:\n",
    "    print(f'{word} - {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая на основе данных таблицы создает список цитат из фильмов, выпущенных после 1995 года.\n",
    "\n",
    "https://ru.wikipedia.org/wiki/100_%D0%B8%D0%B7%D0%B2%D0%B5%D1%81%D1%82%D0%BD%D1%8B%D1%85_%D1%86%D0%B8%D1%82%D0%B0%D1%82_%D0%B8%D0%B7_%D0%B0%D0%BC%D0%B5%D1%80%D0%B8%D0%BA%D0%B0%D0%BD%D1%81%D0%BA%D0%B8%D1%85_%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D0%BE%D0%B2_%D0%B7%D0%B0_100_%D0%BB%D0%B5%D1%82_%D0%BF%D0%BE_%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D0%B8_AFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me the money! Покажи мне деньги! Род Тидвелл Кьюба Гудинг мл. Джерри Магуайер 1996\n",
      "I see dead people. Я вижу мёртвых людей. Коул Сиэр Хэйли Джоэл Осмент Шестое чувство 1999\n",
      "You had me at 'hello'. Я была твоя уже на «здрасьте». Дороти Бойд Рене Зеллвегер Джерри Магуайер 1996\n",
      "My precious. Моя прелесть. Голлум Энди Серкис Властелин колец: Две крепости 2002\n",
      "I’m the king of the world! Я король мира! Джек Доусон Леонардо Ди Каприо Титаник 1997\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "url = 'https://ru.wikipedia.org/wiki/100_%D0%B8%D0%B7%D0%B2%D0%B5%D1%81%D1%82%D0%BD%D1%8B%D1%85_%D1%86%D0%B8%D1%82%D0%B0%D1%82_%D0%B8%D0%B7_%D0%B0%D0%BC%D0%B5%D1%80%D0%B8%D0%BA%D0%B0%D0%BD%D1%81%D0%BA%D0%B8%D1%85_%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D0%BE%D0%B2_%D0%B7%D0%B0_100_%D0%BB%D0%B5%D1%82_%D0%BF%D0%BE_%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D0%B8_AFI'\n",
    "res = requests.get(url).text\n",
    "soup = BeautifulSoup(res,'html.parser')\n",
    "table = soup.find('table', class_='wikitable')\n",
    "header = table.find_all('th')\n",
    "quotes = table.find_all('tr')\n",
    "quotes_after_1995 = []\n",
    "\n",
    "for i in range(1, len(quotes)):\n",
    "    q = quotes[i].get_text()\n",
    "    if int(q[-5:]) > 1995:\n",
    "        quotes_after_1995.append(' '.join(q.split('\\n\\n')[1:]).replace('\\n', ''))\n",
    "        \n",
    "for quote in quotes_after_1995:\n",
    "    print(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая извлекает данные о моделях, конфигурации и стоимости 117-ти ноутбуков, и записывает полученную информацию в csv файл.\n",
    "\n",
    "https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константы для проекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём переменные-КОНСТАНТЫ\n",
    "# 1. Адрес основного хоста\n",
    "HOST = ''\n",
    "# https://chelyabinsk.bani.ru.com/poleznye-statiy/pravila-ekspluatacii-bani-bochki-kak-prodlit-srok-sluzhby/\n",
    "\n",
    "# 2. Адрес страницы для парсинга\n",
    "URL = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "\n",
    "# 3. Создаём словарь заголовков\n",
    "# Это нужно для некоторых сайтов (но не для всех - для этого необязательно)\n",
    "# чтобы показать сайту, что ВЫ НЕ БОТ, а обычный пользователь\n",
    "HEADERS = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отправляем запрос на страницу, передаём в качестве GET-параметра URL-адрес\n",
    "# Сохраним нашу страницу в файл И ЗАКОММЕНТИРУЕМ ЭТУ КОМАНДУ за ненадобностью\n",
    "\n",
    "# res = requests.get(url=URL, headers=HEADERS)\n",
    "# src = res.text\n",
    "# with open('../data/interim/html/index_laptop.html', 'w', encoding='utf-8') as file:\n",
    "#     file.write(src)\n",
    "\n",
    "# Теперь откроем наш файл, прочитаем и сохраним код страницы в переменную:\n",
    "with open('../data/interim/html/index_laptop.html', encoding='utf-8') as file:\n",
    "    src = file.read()\n",
    "# print(src)\n",
    "\n",
    "# f'data/{count}_{category_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы пользоваться методами библиотеки BEAUTIFULSOUP нужно открыть нашу переменную в этой библиотеке\n",
    "# (Преобразовать код в дерево объектов Python)\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "# LXML - это название название парсера (считается самым быстрым)\n",
    "# В стандартной установке его нет (надо устанавливать отдельно)\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "# res = requests.get(src)\n",
    "# soup = BeautifulSoup(res.text,'html.parser')\n",
    "models = soup.find_all('a', class_='title')\n",
    "description = soup.find_all('p', class_='description')\n",
    "prices = soup.find_all('h4', class_='pull-right price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(soup.find_all('a', class_='title'))\n",
    "description = list(soup.find_all('p', class_='description'))\n",
    "prices = list(soup.find_all('h4', class_='pull-right price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(models)\n",
    "type(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель;Описание;Цена\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Модель;Описание;Цена\\n')\n",
    "for m, d, p in zip(models, description, prices):\n",
    "    print(f\"{m};{d};{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель;Описание;Цена\n",
      "\n",
      "Asus VivoBook X441NA-GA190\n",
      "Prestigio SmartBook 133S Dark Grey\n",
      "Prestigio SmartBook 133S Gold\n",
      "Aspire E1-510\n",
      "Lenovo V110-15IAP\n",
      "Lenovo V110-15IAP\n",
      "Hewlett Packard 250 G6 Dark Ash Silver\n",
      "Acer Aspire 3 A315-31 Black\n",
      "Acer Aspire A315-31-C33J\n",
      "Acer Aspire ES1-572 Black\n",
      "Acer Aspire 3 A315-31 Black\n",
      "Acer Aspire 3 A315-21\n",
      "Asus VivoBook Max\n",
      "Asus VivoBook E502NA-GO022T Dark Blue\n",
      "Lenovo ThinkPad E31-80\n",
      "Acer Aspire 3 A315-31 Black\n",
      "Lenovo V110-15ISK\n",
      "Acer Aspire ES1-732 Black\n",
      "Asus VivoBook 15 X540NA-GQ026T\n",
      "Packard 255 G2\n",
      "Asus EeeBook R416NA-FA014T\n",
      "Acer Aspire 3 A315-51\n",
      "Acer Aspire ES1-572 Black\n",
      "Acer Extensa 15 (2540) Black\n",
      "Acer Aspire ES1-572 Black\n",
      "Lenovo V110-15ISK\n",
      "Acer Aspire A315-51-33TG\n",
      "Lenovo V110-15IKB\n",
      "Asus VivoBook 15 X540UA-DM260 Chocolate Black\n",
      "Acer Aspire ES1-572 Black\n",
      "Lenovo V510 Black\n",
      "Acer Aspire ES1-572 Black\n",
      "Lenovo V510 Black\n",
      "Acer Swift 1 SF113-31 Silver\n",
      "Dell Vostro 15\n",
      "Acer Aspire 3 A315-51 Black\n",
      "Dell Vostro 15 (3568) Red\n",
      "Lenovo V510 Black\n",
      "HP 250 G3\n",
      "Acer Spin 5\n",
      "HP 350 G1\n",
      "Aspire E1-572G\n",
      "Pavilion\n",
      "Acer Aspire A515-51-5654\n",
      "Dell Inspiron 15\n",
      "Asus VivoBook S14\n",
      "ProBook\n",
      "Inspiron 15\n",
      "Asus ROG STRIX GL553VD-DM256\n",
      "Acer Nitro 5 AN515-51\n",
      "Asus ROG STRIX GL553VD-DM256\n",
      "Lenovo ThinkPad L570\n",
      "ThinkPad Yoga\n",
      "Lenovo ThinkPad L460\n",
      "Dell Inspiron 15 (7567) Black\n",
      "MSI GL72M 7RDX\n",
      "MSI GL72M 7RDX\n",
      "Asus ROG Strix GL553VD-DM535T\n",
      "Dell Latitude 5280\n",
      "Dell Latitude 5480\n",
      "Lenovo Legion Y520-15IKBM\n",
      "Toshiba Portege Z30-C-16J Grey\n",
      "Acer Predator Helios 300 (PH317-51)\n",
      "Acer Aspire 7 A715-71G\n",
      "Dell Inspiron 17 2in1 (7779) Silver\n",
      "Dell Latitude 5480\n",
      "Lenovo Legion Y520\n",
      "Asus AsusPro Advanced BU401LA-FA271G Dark Grey\n",
      "Acer Nitro 5 AN515-51\n",
      "Dell Latitude 5480\n",
      "Dell Inspiron 15 (7567) Black\n",
      "Dell Latitude 5580\n",
      "Lenovo Legion Y520-15IKBM\n",
      "MSI GP62M 7RDX Leopard\n",
      "Lenovo Yoga 720 Grey\n",
      "Toshiba Portege Z30-C-16L Grey\n",
      "Acer TravelMate P645-S-511A Black\n",
      "Dell Latitude 5580\n",
      "ThinkPad T540p\n",
      "MSI GS63 7RD Stealth\n",
      "Dell Latitude 5480\n",
      "Acer Predator Helios 300 (PH317-51)\n",
      "MSI GL62M 7REX\n",
      "MSI GL62M 7REX2\n",
      "Lenovo Yoga 910 Grey\n",
      "Toshiba Portege X30-D-10J Black/Blue\n",
      "Lenovo IdeaPad Miix 510 Platinum Silver\n",
      "Acer Predator Helios 300 (PH317-51)\n",
      "ThinkPad Yoga\n",
      "Asus VivoBook Pro 15 N580VN-FI006T Gold Metal\n",
      "Dell Latitude 5480\n",
      "Asus ZenBook UX530UX-FY040T Blue\n",
      "ThinkPad X230\n",
      "Asus ROG Strix GL753VE-GC096T\n",
      "Apple MacBook Air 13\"\n",
      "Dell Latitude 5480\n",
      "Hewlett Packard Spectre 13-v106na Dark Ash Silver\n",
      "Dell XPS 13\n",
      "Toshiba Portege Z30-C-16K Grey\n",
      "MSI GL62VR 7RFX\n",
      "Dell Latitude 5480\n",
      "ThinkPad X240\n",
      "Hewlett Packard ProBook 640 G3\n",
      "Apple MacBook Pro 13\" Space Gray\n",
      "Dell Latitude 5580\n",
      "Dell Latitude 5480\n",
      "Dell Latitude 5580\n",
      "Apple MacBook Air 13\"\n",
      "Lenovo ThinkPad T470\n",
      "Lenovo ThinkPad Yoga 370 Black\n",
      "Toshiba Portege X20W-D-10V Black/Blue\n",
      "Asus ASUSPRO B9440UA-GV0279R Gray\n",
      "Lenovo Legion Y720\n",
      "Asus ROG Strix GL702VM-GC146T\n",
      "Asus ROG Strix GL702ZC-GC154T\n",
      "Asus ROG Strix GL702ZC-GC209T\n",
      "Asus ROG Strix SCAR Edition GL503VM-ED115T\n"
     ]
    }
   ],
   "source": [
    "print(f'Модель;Описание;Цена\\n')\n",
    "for m in models:\n",
    "    print(f\"{m['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель;Описание;Цена\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Модель;Описание;Цена\\n')\n",
    "for m, d, p in zip(list(models), list(description), list(prices)):\n",
    "    print(f\"{m['title']};{d.get_text()};{p.get_text()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "models = soup.find_all('a', class_='title')\n",
    "description = soup.find_all('p', class_='description')\n",
    "prices = soup.find_all('h4', class_='pull-right price')\n",
    "\n",
    "with open('../data/interim/csv/laptops.csv', 'w', encoding='utf-8') as file:\n",
    "    # file.write(f'Модель;Описание;Цена\\n')\n",
    "    print(f'Модель;Описание;Цена\\n')\n",
    "    for m, d, p in zip(models, description, prices):\n",
    "        # file.write(f\"{m['title']};{d.get_text()};{p.get_text()}\\n\")\n",
    "        print(f\"{m};{d};{p}\\n\")\n",
    "        file.write(f\"{m};{d};{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"description card-text\">Asus VivoBook X441NA-GA190 Chocolate Black, 14\", Celeron N3450, 4GB, 128GB SSD, Endless OS, ENG kbd</p>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель;Описание;Цена</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Модель;Описание;Цена]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка выполненного задания - загружаем данные из файла CSV\n",
    "laptops = pd.read_csv('../data/interim/csv/laptops.csv')\n",
    "laptops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для скачивания полноразмерных обложек из профилей книг на LiveLib.\n",
    "Обложки открываются после двойного клика по миниатюре:\n",
    "\n",
    "https://www.livelib.ru/book/1002978643-ohotnik-za-tenyu-donato-karrizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-8dc3a9a3dda9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bc-menu__image-wrapper'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mimg_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(?:https\\:)?//.*\\.(?:jpeg)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "url = 'https://www.livelib.ru/book/1002978643-ohotnik-za-tenyu-donato-karrizi'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "sp = soup.find('div', class_='bc-menu__image-wrapper')\n",
    "img_url = re.findall(r'(?:https\\:)?//.*\\.(?:jpeg)', str(sp))[0]\n",
    "response = requests.get(img_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    file_name = url.split('-', 1)[1]\n",
    "    with open(file_name + '.jpeg', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая составляет рейтинг топ-100 лучших триллеров на основе этого списка.\n",
    "\n",
    "https://www.livelib.ru/genre/%D0%A2%D1%80%D0%B8%D0%BB%D0%BB%D0%B5%D1%80%D1%8B/top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "url = 'https://www.livelib.ru/genre/%D0%A2%D1%80%D0%B8%D0%BB%D0%BB%D0%B5%D1%80%D1%8B/top'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "titles = soup.find_all('a', class_='brow-book-name with-cycle')\n",
    "authors = soup.find_all('a', class_='brow-book-author')\n",
    "rating = soup.find_all('span', class_='rating-value stars-color-orange')\n",
    "\n",
    "i = 1\n",
    "for t, a, r in zip(titles, authors, rating):\n",
    "    print(f'{i}. \"{t.get_text()}\", {a.get_text()} - {r.get_text()}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая составляет топ-20 языков программирования на основе рейтинга популярности TIOBE.\n",
    "\n",
    "https://www.tiobe.com/tiobe-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Python: 21.90%\n",
      "2. C++: 11.60%\n",
      "3. Java: 10.51%\n",
      "4. C: 8.38%\n",
      "5. C#: 5.62%\n",
      "6. JavaScript: 3.54%\n",
      "7. Visual Basic: 2.35%\n",
      "8. Go: 2.02%\n",
      "9. Fortran: 1.80%\n",
      "10. Delphi/Object Pascal: 1.68%\n",
      "11. SQL: 1.64%\n",
      "12. MATLAB: 1.48%\n",
      "13. Rust: 1.45%\n",
      "14. Scratch: 1.41%\n",
      "15. PHP: 1.21%\n",
      "16. Assembly language: 1.13%\n",
      "17. R: 1.09%\n",
      "18. Ruby: 0.99%\n",
      "19. COBOL: 0.99%\n",
      "20. Swift: 0.98%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "url = 'https://www.tiobe.com/tiobe-index/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "tree = html.fromstring(page.content)\n",
    "languages, rating = [], []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    languages.append(tree.xpath(f'//*[@id=\"top20\"]/tbody/tr[{i}]/td[5]/text()')[0])\n",
    "    rating.append(tree.xpath(f'//*[@id=\"top20\"]/tbody/tr[{i}]/td[6]/text()')[0])\n",
    "i = 1\n",
    "\n",
    "for l, r in zip(languages, rating):\n",
    "    print(f'{i}. {l}: {r}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для получения рейтинга 250 лучших фильмов по версии IMDb. Названия должны быть на русском языке.\n",
    "\n",
    "https://www.imdb.com/chart/top/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-4829a4998db7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m251\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/a/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0myear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/span/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[3]/strong/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "url = 'https://www.imdb.com/chart/top/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\",\n",
    "           \"Accept-Language\": \"ru-RU\"}\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "tree = html.fromstring(page.content)\n",
    "movies, year, rating = [], [], []\n",
    "\n",
    "for i in range(1, 251):\n",
    "    movies.append(tree.xpath(f'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/a/text()')[0])\n",
    "    year.append(tree.xpath(f'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/span/text()')[0])\n",
    "    rating.append(tree.xpath(f'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[3]/strong/text()')[0])\n",
    "i = 1\n",
    "\n",
    "for m, y, r in zip(movies, year, rating):\n",
    "    print(f'{i}. {m}, {y}, {r}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая сохраняет в текстовый файл данные о фэнтези фильмах с 10 первых страниц соответствующего раздела IMDb.\n",
    "Если у фильма/сериала еще нет рейтинга, следует указать N/A.\n",
    "\n",
    "https://www.imdb.com/search/title/?genres=fantasy\n",
    "\n",
    "Ожидаемый результат в файле fantasy.txt – 500 записей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-38a45a5e6988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'N/A'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mtitles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/a/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0myear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/span[2]/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/div/div[1]/strong/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import mechanicalsoup\n",
    "from lxml import html\n",
    "import time\n",
    "\n",
    "url = 'https://www.imdb.com/search/title/?genres=fantasy'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\",\n",
    "           \"Accept-Language\": \"ru-RU\"}\n",
    "\n",
    "browser = mechanicalsoup.StatefulBrowser()\n",
    "j = 51\n",
    "\n",
    "for _ in range(10):\n",
    "    browser.open(url)\n",
    "    page = requests.get(url, headers=headers)\n",
    "    tree = html.fromstring(page.content)\n",
    "    titles, year, rating = [], [], []\n",
    "    for i in range(1, 51):\n",
    "        if tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/p[1]/b/text()') != []:\n",
    "            titles.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/a/text()')[0])\n",
    "            year.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/span[2]/text()')[0])\n",
    "            rating.append('N/A')\n",
    "        else:\n",
    "            titles.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/a/text()')[0])\n",
    "            year.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/span[2]/text()')[0])\n",
    "            rating.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/div/div[1]/strong/text()')[0])\n",
    "    with open('fantasy.txt', 'a', encoding='utf-8') as file:\n",
    "        for t, y, r in zip(titles, year, rating):\n",
    "            file.write(f'{t}, {y}, {r}\\n')\n",
    "\n",
    "    time.sleep(2)    \n",
    "    lnk = browser.follow_link('start=' + str(j))\n",
    "    url = browser.url\n",
    "    j += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для получения главных новостей (на русском) с портала Habr.\n",
    "Каждый заголовок должен сопровождаться ссылкой на полный текст новости.\n",
    "\n",
    "https://habr.com/ru/news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://habr.com/ru/news/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\",\n",
    "           \"Accept-Language\": \"ru-RU\"}\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "articles = soup.find_all('a', class_='tm-article-snippet__title-link')\n",
    "\n",
    "for a in articles:\n",
    "    print(f'{a.get_text()}\\nhttps://habr.com{a.get(\"href\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
