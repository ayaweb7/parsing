{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Самоучитель по Python для начинающих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 17: Основы скрапинга и парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужные нам библиотеки.\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование объекта datetime :\n",
    "from datetime import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регулярные выражения\n",
    "from urllib.request import urlopen\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы с HTML кодом в Python используют модуль etree:\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MechanicalSoup\n",
      "  Downloading MechanicalSoup-1.3.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.7 in c:\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.9.1)\n",
      "Requirement already satisfied: lxml in c:\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.5.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\anaconda3\\lib\\site-packages (from MechanicalSoup) (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.7->MechanicalSoup) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests>=2.22.0->MechanicalSoup) (3.0.4)\n",
      "Installing collected packages: MechanicalSoup\n",
      "Successfully installed MechanicalSoup-1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Библиотека для имитации действий пользователя:\n",
    "\n",
    "!pip install MechanicalSoup\n",
    "# import mechanicalsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы с динамическим контентом в Python нужно установить:\n",
    "\n",
    "# - Модуль Selenium.\n",
    "# - Драйвер Selenium WebDriver для браузера.\n",
    "\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.14.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Библиотека для имитации действий пользователя:\n",
    "\n",
    "# !pip install selenium\n",
    "# import selenium\n",
    "selenium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ChromeDriverManager in c:\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: pywin32 in c:\\anaconda3\\lib\\site-packages (from ChromeDriverManager) (227)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from ChromeDriverManager) (2.24.0)\n",
      "Requirement already satisfied: pypiwin32 in c:\\anaconda3\\lib\\site-packages (from ChromeDriverManager) (223)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->ChromeDriverManager) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install ChromeDriverManager\n",
    "# ChromeDriverManager.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChromeDriverManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0ef7914e43a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ChromeDriverManager' is not defined"
     ]
    }
   ],
   "source": [
    "ChromeDriverManager.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement webdriver_manager.chrome (from versions: none)\n",
      "ERROR: No matching distribution found for webdriver_manager.chrome\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager.chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webdriver_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e2f2071796ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwebdriver_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchrome\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'webdriver_manager'"
     ]
    }
   ],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бывает, что даже после установки оптимальной версии драйвера интерпретатор Python возвращает ошибку OSError:\n",
    "# [WinError 216] Версия \"%1\" не совместима с версией Windows.\n",
    "# В этом случае нужно воспользоваться модулем webdriver-manager,\n",
    "# который самостоятельно установит подходящий драйвер для нужного браузера:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use with Chrome\n",
    "# selenium 3\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium 4\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполняем предыдущую ячейку ПОСТРОЧНО !!!\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service as ChromeService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.24.0)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from webdriver-manager) (20.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (2.4.7)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install webdriver-manager\n",
    "# from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'service'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-9e67e88712cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mChromeService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'service'"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научимся извлекать данные из статического и динамического контента с помощью:\n",
    "регулярных выражений, XPath, BeautifulSoup, MechanicalSoup и Selenium.\n",
    "\n",
    "В конце статьи – код 10 скриптов для скрапинга данных и изображений с Wikipedia, Habr, LiveLib, IMDb и TIOBE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веб-скрапинг – это процесс автоматического сбора информации из онлайн-источников.\n",
    "Для выбора нужных сведений из массива «сырых» данных, полученных в ходе скрапинга, нужна дальнейшая обработка – парсинг.\n",
    "В процессе парсинга выполняются синтаксический анализ, разбор и очистка данных.\n",
    "Результат парсинга – очищенные, упорядоченные, структурированные данные, представленные в формате, понятном конечному пользователю (или приложению)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипты для скрапинга создают определенную нагрузку на сайт, с которого они собирают данные – могут, например, посылать чрезмерное количество GET запросов к серверу.\n",
    "Это одна из причин, по которой скрапинг относится к спорным видам деятельности.\n",
    "Чтобы не выходить за рамки сетевого этикета, необходимо всегда соблюдать главные правила сбора публичной информации:\n",
    "\n",
    "- Если на сайте есть API, нужно запрашивать данные у него.\n",
    "- Частота и количество GET запросов должны быть разумными.\n",
    "- Следует передавать информацию о клиенте в User-Agent.\n",
    "- Если на сайте есть личные данные пользователей, необходимо учитывать настройки приватности в robots.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо отметить, что универсальных рецептов скрапинга и парсинга не существует.\n",
    "Это связано со следующими причинами:\n",
    "\n",
    "- Некоторые сервисы активно блокируют скраперов. Динамическая смена прокси не всегда помогает решить эту проблему.\n",
    "\n",
    "- Контент многих современных сайтов генерируется динамически – результат обычного GET запроса из приложения к таким сайтам вернется практически пустым. Эта проблема решается с помощью Selenium WebDriver либо MechanicalSoup, которые имитируют действия браузера и пользователя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для извлечения данных со страниц с четкой, стандартной структурой эффективнее использовать язык запросов XPath.\n",
    "\n",
    "И напротив, для получения нужной информации с нестандартных страниц с произвольным синтаксисом лучше использовать средства библиотеки BeautifulSoup.\n",
    "\n",
    "Ниже мы подробно рассмотрим оба подхода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры скрапинга и парсинга на Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экосистема Python располагает множеством инструментов для скрапинга и парсинга.\n",
    "Начнем с самого простого примера – получения веб-страницы и извлечения из ее кода ссылки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скрапинг содержимого страницы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://example.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Воспользуемся модулем urllib.request стандартной библиотеки urllib\n",
    "# для получения исходного кода одностраничного сайта example.com:\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = 'http://example.com'\n",
    "page = urlopen(url)\n",
    "\n",
    "print(page.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Example Domain</title>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "<style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "<div>\n",
      "<h1>Example Domain</h1>\n",
      "<p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "<p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Точно такой же результат можно получить с помощью requests:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://example.com'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот результат – те самые сырые данные, которые нужно обработать (подвергнуть парсингу), чтобы извлечь из них нужную информацию, например, адрес указанной на странице ссылки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Парсинг полученных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечь адрес ссылки можно 4 разными способами – с помощью:\n",
    "\n",
    "- Методов строк.\n",
    "- Регулярного выражения.\n",
    "- Запроса XPath.\n",
    "- Обработки BeautifulSoup.\n",
    "\n",
    "Рассмотрим все эти способы по порядку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы строк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это самый трудоемкий способ – для извлечения каждого элемента нужно определить 2 индекса – начало и конец вхождения.\n",
    "При этом к индексу вхождения надо добавить длину стартового фрагмента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167\n",
      "1203\n",
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# \n",
    "url = 'http://example.com/'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "\n",
    "# \n",
    "start = html_code.find('href=\"') + 6\n",
    "end = html_code.find('\">More')\n",
    "link = html_code[start:end]\n",
    "\n",
    "print(start)\n",
    "print(end)\n",
    "\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Регулярное выражение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущей главе мы подробно рассматривали способы извлечения конкретных подстрок из текста.\n",
    "https://proglib.io/p/samouchitel-po-python-dlya-nachinayushchih-chast-16-regulyarnye-vyrazheniya-2023-02-20\n",
    "\n",
    "Точно так же регулярные выражения можно использовать для поиска данных в исходном коде страниц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.iana.org/domains/example']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "# \n",
    "url = 'http://example.com/'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "\n",
    "# \n",
    "link = r'(https?://\\S+)(?=\")'\n",
    "\n",
    "print(re.findall(link, html_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запрос XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Язык запросов XPath (XML Path Language) позволяет извлекать данные из определенных узлов XML-документа.\n",
    "Для работы с HTML кодом в Python используют модуль etree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "\n",
    "# \n",
    "url = 'http://example.com/'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "tree = etree.HTML(html_code)\n",
    "\n",
    "print(tree.xpath(\"/html/body/div/p[2]/a/@href\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы узнать путь к нужному элементу страницы, в браузерах Chrome и FireFox надо кликнуть правой кнопкой по элементу и выбрать «Просмотреть код», после чего откроется консоль.\n",
    "В консоли по интересующему элементу нужно еще раз кликнуть правой кнопкой, выбрать «Копировать», а затем – копировать путь XPath:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В приведенном выше примере для извлечения ссылки к пути /html/body/div/p[2]/a/\n",
    "мы добавили указание для получения значения ссылки @href, и индекс [0], поскольку результат возвращается в виде списка.\n",
    "\n",
    "###### Если @href заменить на text(), программа вернет текст ссылки, а не сам URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More information...\n"
     ]
    }
   ],
   "source": [
    "print(tree.xpath(\"/html/body/div/p[2]/a/text()\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсер BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярные выражения и XPath предоставляют огромные возможности для извлечения нужной информации из кода страниц, но у них есть свои недостатки:\n",
    "составлять Regex-шаблоны сложно, а запросы XPath хорошо работают только на страницах с безупречной, стандартной структурой.\n",
    "К примеру, страницы Википедии не отличаются идеальной структурой, и использование XPath для извлечения нужной информации из определенных элементов статей, таких как таблицы infobox, часто оказывается неэффективным.\n",
    "\n",
    "##### В этом случае оптимальным вариантом становится BeautifulSoup, специально разработанный для парсинга HTML-кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Библиотека BeautifulSoup не входит в стандартный набор Python, ее нужно установить самостоятельно:\n",
    "        \n",
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В приведенном ниже примере мы будем извлекать из исходного кода страницы уникальные ссылки, за исключением внутренних:\n",
    "\n",
    "https://webscraper.io/test-sites/e-commerce/allinone/phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/webscraperio\n",
      "https://forum.webscraper.io/\n",
      "https://cloud.webscraper.io/\n",
      "https://www.facebook.com/webscraperio/\n",
      "https://chromewebstore.google.com/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en\n",
      "https://youtube.com/@WebScraper/videos\n",
      "https://webscraper.io/downloads/Web_Scraper_Media_Kit.zip\n",
      "https://lv.linkedin.com/company/web-scraper\n",
      "https://status.webscraper.io/\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# \n",
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/phones'\n",
    "page = urlopen(url)\n",
    "html = page.read().decode('utf-8')\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# \n",
    "links = set()\n",
    "for link in soup.find_all('a'):\n",
    "    l = link.get('href')\n",
    "    if l != None and l.startswith('https'):\n",
    "        links.add(l)\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/webscraperio\n",
      "https://forum.webscraper.io/\n",
      "https://cloud.webscraper.io/\n",
      "https://www.facebook.com/webscraperio/\n",
      "https://chromewebstore.google.com/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en\n",
      "https://youtube.com/@WebScraper/videos\n",
      "https://webscraper.io/downloads/Web_Scraper_Media_Kit.zip\n",
      "https://lv.linkedin.com/company/web-scraper\n",
      "https://status.webscraper.io/\n"
     ]
    }
   ],
   "source": [
    "# При использовании XPath точно такой же результат даст следующий скрипт:\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "\n",
    "# \n",
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/phones'\n",
    "page = urlopen(url)\n",
    "html_code = page.read().decode('utf-8')\n",
    "tree = etree.HTML(html_code)\n",
    "sp = tree.xpath(\"//li/a/@href\")\n",
    "\n",
    "# \n",
    "links = set()\n",
    "for link in sp:\n",
    "    if link.startswith('http'):\n",
    "        links.add(link)\n",
    "\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имитация действий пользователя в браузере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При скрапинге сайтов очень часто требуется авторизация, нажатие кнопок «Читать дальше», переход по ссылкам, отправка форм, прокручивание ленты и так далее.\n",
    "Отсюда возникает необходимость имитации действий пользователя.\n",
    "Как правило, для этих целей используют Selenium, однако есть и более легкое решение – библиотека MechanicalSoup:\n",
    "\n",
    "###### pip install MechanicalSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, MechanicalSoup исполняет роль браузера без графического интерфейса.\n",
    "Помимо имитации нужного взаимодействия с элементами страниц,\n",
    "MechanicalSoup также парсит HTML-код, используя для этого все функции BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся тестовым сайтом http://httpbin.org/, на котором есть возможность отправки формы заказа пиццы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<input name=\"custname\"/>\n",
      "<input name=\"custtel\" type=\"tel\"/>\n",
      "<input name=\"custemail\" type=\"email\"/>\n",
      "<input name=\"size\" type=\"radio\" value=\"small\"/>\n",
      "<input name=\"size\" type=\"radio\" value=\"medium\"/>\n",
      "<input name=\"size\" type=\"radio\" value=\"large\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"bacon\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"cheese\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"onion\"/>\n",
      "<input name=\"topping\" type=\"checkbox\" value=\"mushroom\"/>\n",
      "<input max=\"21:00\" min=\"11:00\" name=\"delivery\" step=\"900\" type=\"time\"/>\n",
      "<textarea name=\"comments\"></textarea>\n",
      "<button>Submit order</button>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "\n",
    "# \n",
    "browser = mechanicalsoup.StatefulBrowser()\n",
    "browser.open(\"http://httpbin.org/\")\n",
    "browser.follow_link(\"forms\")\n",
    "browser.select_form('form[action=\"/post\"]')\n",
    "\n",
    "print(browser.form.print_summary())\n",
    "\n",
    "# В приведенном выше примере браузер MechanicalSoup перешел по внутренней ссылке http://httpbin.org/forms/post\n",
    "# и вернул описание полей ввода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перейдем к имитации заполнения формы:\n",
    "        \n",
    "browser[\"custname\"] = \"Best Customer\"\n",
    "browser[\"custtel\"] = \"+7 916 123 45 67\"\n",
    "browser[\"custemail\"] = \"trex@example.com\"\n",
    "browser[\"size\"] = \"large\"\n",
    "browser[\"topping\"] = (\"cheese\", \"mushroom\")\n",
    "browser[\"comments\"] = \"Add more cheese, plz. More than the last time!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь форму можно отправить:\n",
    "        \n",
    "response = browser.submit_selected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"comments\": \"Add more cheese, plz. More than the last time!\", \n",
      "    \"custemail\": \"trex@example.com\", \n",
      "    \"custname\": \"Best Customer\", \n",
      "    \"custtel\": \"+7 916 123 45 67\", \n",
      "    \"delivery\": \"\", \n",
      "    \"size\": \"large\", \n",
      "    \"topping\": [\n",
      "      \"cheese\", \n",
      "      \"mushroom\"\n",
      "    ]\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"191\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"Referer\": \"http://httpbin.org/forms/post\", \n",
      "    \"User-Agent\": \"python-requests/2.24.0 (MechanicalSoup/1.3.0)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-67155811-78bd5d122a1637122c7af07f\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"94.25.225.163\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Результат можно вывести с помощью print(response.text):\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скрапинг и парсинг динамического контента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все примеры, которые мы рассмотрели выше, отлично работают на статических страницах.\n",
    "Однако на множестве платформ используется динамический подход к генерации и загрузке контента –\n",
    "к примеру, для просмотра всех доступных товаров в онлайн-магазине страницу нужно не только открыть, но и прокрутить до футера. Для работы с динамическим контентом в Python нужно установить:\n",
    "\n",
    "- Модуль Selenium.\n",
    "- Драйвер Selenium WebDriver для браузера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ ИСПОЛЬЗОВАТЬ  ДАННЫЙ  КОД  ДЛЯ  ПАРСИНГА  САЙТОВ $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "# Use with Chrome\n",
    "# selenium 3\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ ИСПОЛЬЗОВАТЬ  ДАННЫЙ  КОД  ДЛЯ  ПАРСИНГА  САЙТОВ $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если установка прошла успешно, выполнение этого кода приведет к автоматическому открытию страницы:\n",
    "# from selenium import webdriver\n",
    "\n",
    "\n",
    "# driver = webdriver.Chrome() # или webdriver.Firefox()\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get('https://google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Имитация прокрутки страницы и парсинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера скрапинга и парсинга динамического сайта мы воспользуемся разделом тестового онлайн-магазина.\n",
    "Здесь расположены карточки с информацией о планшетах.\n",
    "Карточки загружаются ряд за рядом при прокрутке страницы:\n",
    "\n",
    "https://webscraper.io/test-sites/e-commerce/scroll/computers/tablets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока страница не прокручена, полный HTML-код с информацией о планшетах получить невозможно.\n",
    "Для имитации прокрутки мы воспользуемся скриптом 'window.scrollTo(0, document.body.scrollHeight);'.\n",
    "Цены планшетов находятся в тегах \"H4\" класса pull-right price, а названия моделей – в тексте ссылок \"A\" класса title.\n",
    "Готовый код выглядит так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_line_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             self.process = subprocess.Popen(cmd, env=self.env,\n\u001b[0m\u001b[0;32m     73\u001b[0m                                             \u001b[0mclose_fds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Windows'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Не удается найти указанный файл",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-5a62c0f09c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# driver = webdriver.Chrome(service=service)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:\\Program Files\\Chrome Driver\\chromedriver.exe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://webscraper.io/test-sites/e-commerce/scroll/computers/tablets'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             log_path=service_log_path)\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 raise WebDriverException(\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m     83\u001b[0m                         os.path.basename(self.path), self.start_error_message)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'chromedriver.exe' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Оригинальная функция к данной задаче \n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager(cache_valid_range=10).install()))\n",
    "\n",
    "# Функция полученная несколько ячеек выше с добавлением \"cache_valid_range=10\":\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(ChromeDriverManager(cache_valid_range=10).install())\n",
    "\n",
    "\n",
    "url = 'https://webscraper.io/test-sites/e-commerce/scroll/computers/tablets'\n",
    "driver.get(url)\n",
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "time.sleep(5) \n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "prices = soup.find_all('h4', class_='pull-right price')\n",
    "models = soup.find_all('a', class_='title')\n",
    "\n",
    "for model, price in zip(models, prices):\n",
    "    m = model.get_text()\n",
    "    p = price.get_text()\n",
    "    print(f'Планшет {m}, цена - {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для получения названий последних статей из блога издательства O’Reilly:\n",
    "\n",
    "https://www.oreilly.com/radar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI in the Real World: The Future of Programming with Matt Welsh\n",
      "The State of Security in 2024\n",
      "Beyond Imitation\n",
      "Henry Ford Does AI\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "url = 'https://www.oreilly.com/radar/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "sp = soup.find_all('h2', class_='post-title')\n",
    "for post in sp:\n",
    "    print(post.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для определения 10 слов, которые чаще всего встречаются в тексте сказки «Колобок» (без учета регистра). Предлоги учитывать не нужно.\n",
    "\n",
    "https://azku.ru/russkie-narodnie-skazki/kolobok.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "колобок - 32\n",
      "ушел - 14\n",
      "тебя - 7\n",
      "коробу - 6\n",
      "сусеку - 6\n",
      "сметане - 5\n",
      "масле - 5\n",
      "покатился - 4\n",
      "катится - 4\n",
      "навстречу - 4\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "url = 'https://azku.ru/russkie-narodnie-skazki/kolobok.html'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "res = soup.find_all('div', class_='entry-content')\n",
    "text = res[0].get_text().split('Мне нравится')[0]\n",
    "sp = re.sub(r'[-,.?!\"—:]', ' ', text).lower().split()\n",
    "\n",
    "result_dict = {word: sp.count(word) for word in sp if len(word) > 3}\n",
    "max_values = sorted(result_dict.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "\n",
    "for word, number in max_values:\n",
    "    print(f'{word} - {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая на основе данных таблицы создает список цитат из фильмов, выпущенных после 1995 года.\n",
    "\n",
    "https://ru.wikipedia.org/wiki/100_%D0%B8%D0%B7%D0%B2%D0%B5%D1%81%D1%82%D0%BD%D1%8B%D1%85_%D1%86%D0%B8%D1%82%D0%B0%D1%82_%D0%B8%D0%B7_%D0%B0%D0%BC%D0%B5%D1%80%D0%B8%D0%BA%D0%B0%D0%BD%D1%81%D0%BA%D0%B8%D1%85_%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D0%BE%D0%B2_%D0%B7%D0%B0_100_%D0%BB%D0%B5%D1%82_%D0%BF%D0%BE_%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D0%B8_AFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me the money! Покажи мне деньги! Род Тидвелл Кьюба Гудинг мл. Джерри Магуайер 1996\n",
      "I see dead people. Я вижу мёртвых людей. Коул Сиэр Хэйли Джоэл Осмент Шестое чувство 1999\n",
      "You had me at 'hello'. Я была твоя уже на «здрасьте». Дороти Бойд Рене Зеллвегер Джерри Магуайер 1996\n",
      "My precious. Моя прелесть. Голлум Энди Серкис Властелин колец: Две крепости 2002\n",
      "I’m the king of the world! Я король мира! Джек Доусон Леонардо Ди Каприо Титаник 1997\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "url = 'https://ru.wikipedia.org/wiki/100_%D0%B8%D0%B7%D0%B2%D0%B5%D1%81%D1%82%D0%BD%D1%8B%D1%85_%D1%86%D0%B8%D1%82%D0%B0%D1%82_%D0%B8%D0%B7_%D0%B0%D0%BC%D0%B5%D1%80%D0%B8%D0%BA%D0%B0%D0%BD%D1%81%D0%BA%D0%B8%D1%85_%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D0%BE%D0%B2_%D0%B7%D0%B0_100_%D0%BB%D0%B5%D1%82_%D0%BF%D0%BE_%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D0%B8_AFI'\n",
    "res = requests.get(url).text\n",
    "soup = BeautifulSoup(res,'html.parser')\n",
    "table = soup.find('table', class_='wikitable')\n",
    "header = table.find_all('th')\n",
    "quotes = table.find_all('tr')\n",
    "quotes_after_1995 = []\n",
    "\n",
    "for i in range(1, len(quotes)):\n",
    "    q = quotes[i].get_text()\n",
    "    if int(q[-5:]) > 1995:\n",
    "        quotes_after_1995.append(' '.join(q.split('\\n\\n')[1:]).replace('\\n', ''))\n",
    "        \n",
    "for quote in quotes_after_1995:\n",
    "    print(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая извлекает данные о моделях, конфигурации и стоимости 117-ти ноутбуков, и записывает полученную информацию в csv файл.\n",
    "\n",
    "https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "models = soup.find_all('a', class_='title')\n",
    "description = soup.find_all('p', class_='description')\n",
    "prices = soup.find_all('h4', class_='pull-right price')\n",
    "\n",
    "with open('laptops.csv', 'w', encoding='utf-8') as file:\n",
    "    file.write(f'Модель;Описание;Цена\\n')\n",
    "    for m, d, p in zip(models, description, prices):\n",
    "        file.write(f\"{m['title']};{d.get_text()};{p.get_text()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель;Описание;Цена</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Модель;Описание;Цена]\n",
       "Index: []"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка выполненного задания - загружаем данные из файла CSV\n",
    "laptops = pd.read_csv('laptops.csv')\n",
    "laptops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для скачивания полноразмерных обложек из профилей книг на LiveLib.\n",
    "Обложки открываются после двойного клика по миниатюре:\n",
    "\n",
    "https://www.livelib.ru/book/1002978643-ohotnik-za-tenyu-donato-karrizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-8dc3a9a3dda9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bc-menu__image-wrapper'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mimg_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(?:https\\:)?//.*\\.(?:jpeg)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "url = 'https://www.livelib.ru/book/1002978643-ohotnik-za-tenyu-donato-karrizi'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "sp = soup.find('div', class_='bc-menu__image-wrapper')\n",
    "img_url = re.findall(r'(?:https\\:)?//.*\\.(?:jpeg)', str(sp))[0]\n",
    "response = requests.get(img_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    file_name = url.split('-', 1)[1]\n",
    "    with open(file_name + '.jpeg', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая составляет рейтинг топ-100 лучших триллеров на основе этого списка.\n",
    "\n",
    "https://www.livelib.ru/genre/%D0%A2%D1%80%D0%B8%D0%BB%D0%BB%D0%B5%D1%80%D1%8B/top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "url = 'https://www.livelib.ru/genre/%D0%A2%D1%80%D0%B8%D0%BB%D0%BB%D0%B5%D1%80%D1%8B/top'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "titles = soup.find_all('a', class_='brow-book-name with-cycle')\n",
    "authors = soup.find_all('a', class_='brow-book-author')\n",
    "rating = soup.find_all('span', class_='rating-value stars-color-orange')\n",
    "\n",
    "i = 1\n",
    "for t, a, r in zip(titles, authors, rating):\n",
    "    print(f'{i}. \"{t.get_text()}\", {a.get_text()} - {r.get_text()}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая составляет топ-20 языков программирования на основе рейтинга популярности TIOBE.\n",
    "\n",
    "https://www.tiobe.com/tiobe-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Python: 21.90%\n",
      "2. C++: 11.60%\n",
      "3. Java: 10.51%\n",
      "4. C: 8.38%\n",
      "5. C#: 5.62%\n",
      "6. JavaScript: 3.54%\n",
      "7. Visual Basic: 2.35%\n",
      "8. Go: 2.02%\n",
      "9. Fortran: 1.80%\n",
      "10. Delphi/Object Pascal: 1.68%\n",
      "11. SQL: 1.64%\n",
      "12. MATLAB: 1.48%\n",
      "13. Rust: 1.45%\n",
      "14. Scratch: 1.41%\n",
      "15. PHP: 1.21%\n",
      "16. Assembly language: 1.13%\n",
      "17. R: 1.09%\n",
      "18. Ruby: 0.99%\n",
      "19. COBOL: 0.99%\n",
      "20. Swift: 0.98%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "url = 'https://www.tiobe.com/tiobe-index/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"}\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "tree = html.fromstring(page.content)\n",
    "languages, rating = [], []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    languages.append(tree.xpath(f'//*[@id=\"top20\"]/tbody/tr[{i}]/td[5]/text()')[0])\n",
    "    rating.append(tree.xpath(f'//*[@id=\"top20\"]/tbody/tr[{i}]/td[6]/text()')[0])\n",
    "i = 1\n",
    "\n",
    "for l, r in zip(languages, rating):\n",
    "    print(f'{i}. {l}: {r}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для получения рейтинга 250 лучших фильмов по версии IMDb. Названия должны быть на русском языке.\n",
    "\n",
    "https://www.imdb.com/chart/top/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-4829a4998db7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m251\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/a/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0myear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/span/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[3]/strong/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "url = 'https://www.imdb.com/chart/top/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\",\n",
    "           \"Accept-Language\": \"ru-RU\"}\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "tree = html.fromstring(page.content)\n",
    "movies, year, rating = [], [], []\n",
    "\n",
    "for i in range(1, 251):\n",
    "    movies.append(tree.xpath(f'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/a/text()')[0])\n",
    "    year.append(tree.xpath(f'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[2]/span/text()')[0])\n",
    "    rating.append(tree.xpath(f'//*[@id=\"main\"]/div/span/div/div/div[3]/table/tbody/tr[{i}]/td[3]/strong/text()')[0])\n",
    "i = 1\n",
    "\n",
    "for m, y, r in zip(movies, year, rating):\n",
    "    print(f'{i}. {m}, {y}, {r}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу, которая сохраняет в текстовый файл данные о фэнтези фильмах с 10 первых страниц соответствующего раздела IMDb.\n",
    "Если у фильма/сериала еще нет рейтинга, следует указать N/A.\n",
    "\n",
    "https://www.imdb.com/search/title/?genres=fantasy\n",
    "\n",
    "Ожидаемый результат в файле fantasy.txt – 500 записей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-38a45a5e6988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'N/A'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mtitles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/a/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0myear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/span[2]/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/div/div[1]/strong/text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import mechanicalsoup\n",
    "from lxml import html\n",
    "import time\n",
    "\n",
    "url = 'https://www.imdb.com/search/title/?genres=fantasy'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\",\n",
    "           \"Accept-Language\": \"ru-RU\"}\n",
    "\n",
    "browser = mechanicalsoup.StatefulBrowser()\n",
    "j = 51\n",
    "\n",
    "for _ in range(10):\n",
    "    browser.open(url)\n",
    "    page = requests.get(url, headers=headers)\n",
    "    tree = html.fromstring(page.content)\n",
    "    titles, year, rating = [], [], []\n",
    "    for i in range(1, 51):\n",
    "        if tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/p[1]/b/text()') != []:\n",
    "            titles.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/a/text()')[0])\n",
    "            year.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/span[2]/text()')[0])\n",
    "            rating.append('N/A')\n",
    "        else:\n",
    "            titles.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/a/text()')[0])\n",
    "            year.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/h3/span[2]/text()')[0])\n",
    "            rating.append(tree.xpath(f'//*[@id=\"main\"]/div/div[3]/div/div[{i}]/div[3]/div/div[1]/strong/text()')[0])\n",
    "    with open('fantasy.txt', 'a', encoding='utf-8') as file:\n",
    "        for t, y, r in zip(titles, year, rating):\n",
    "            file.write(f'{t}, {y}, {r}\\n')\n",
    "\n",
    "    time.sleep(2)    \n",
    "    lnk = browser.follow_link('start=' + str(j))\n",
    "    url = browser.url\n",
    "    j += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите программу для получения главных новостей (на русском) с портала Habr.\n",
    "Каждый заголовок должен сопровождаться ссылкой на полный текст новости.\n",
    "\n",
    "https://habr.com/ru/news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://habr.com/ru/news/'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\",\n",
    "           \"Accept-Language\": \"ru-RU\"}\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "articles = soup.find_all('a', class_='tm-article-snippet__title-link')\n",
    "\n",
    "for a in articles:\n",
    "    print(f'{a.get_text()}\\nhttps://habr.com{a.get(\"href\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
